{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoEncoder Training\n",
    "This notebook creates and trains an autoencoder on the MNIST dataset. It more or less does everything you need, and all you have to do is run the cells.\n",
    "\n",
    "Might later add robustness to the autoencoder class depending on how this compression goes with the quantum learning. Hopefully that won't break this notebook. I'll try to update this notebook if it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "from MNISTData import MNISTData\n",
    "from AutoEncoder import AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MNISTData()\n",
    "train_loader = data.get_train_loader()\n",
    "test_loader = data.get_test_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = AutoEncoder([256, 64, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 256]         200,960\n",
      "              ReLU-2                  [-1, 256]               0\n",
      "            Linear-3                   [-1, 64]          16,448\n",
      "              ReLU-4                   [-1, 64]               0\n",
      "            Linear-5                   [-1, 10]             650\n",
      "              ReLU-6                   [-1, 10]               0\n",
      "            Linear-7                   [-1, 64]             704\n",
      "              ReLU-8                   [-1, 64]               0\n",
      "            Linear-9                  [-1, 256]          16,640\n",
      "             ReLU-10                  [-1, 256]               0\n",
      "           Linear-11                  [-1, 784]         201,488\n",
      "          Sigmoid-12                  [-1, 784]               0\n",
      "================================================================\n",
      "Total params: 436,890\n",
      "Trainable params: 436,890\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 1.67\n",
      "Estimated Total Size (MB): 1.69\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(autoencoder, input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = \"./autoencoder_models\"\n",
    "def save_model(model, model_name=\"ae.pt\", message=None, fig_to_save=None):\n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "    subdirectory_name = str(int(time.time()))\n",
    "    subdirectory_path = os.path.join(save_directory, subdirectory_name)\n",
    "    if not os.path.exists(subdirectory_path):\n",
    "        os.makedirs(subdirectory_path)\n",
    "    save_path = os.path.join(subdirectory_path, model_name)\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    layer_sizes_save_path = os.path.join(subdirectory_path, \"layer_sizes.txt\")\n",
    "    with open(layer_sizes_save_path, 'w') as f:\n",
    "        f.write(str(model.layer_sizes) + \"\\n\")\n",
    "    if message is not None:\n",
    "        message_save_path = os.path.join(subdirectory_path, \"message.txt\")\n",
    "        with open(message_save_path, 'w') as f:\n",
    "            f.write(message + \"\\n\")\n",
    "    if fig_to_save is not None:\n",
    "        fig_save_path = os.path.join(subdirectory_path, \"generated_examples.png\")\n",
    "        fig_to_save.savefig(fig_save_path)\n",
    "    print(\"everything was saved to %s\" % subdirectory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-3, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 0.264\n",
      "[1,   400] loss: 0.231\n",
      "epoch [1/100], loss:0.2317\n",
      "[2,   200] loss: 0.231\n",
      "[2,   400] loss: 0.231\n",
      "epoch [2/100], loss:0.2316\n",
      "[3,   200] loss: 0.231\n",
      "[3,   400] loss: 0.231\n",
      "epoch [3/100], loss:0.2314\n",
      "[4,   200] loss: 0.231\n",
      "[4,   400] loss: 0.231\n",
      "epoch [4/100], loss:0.2315\n",
      "[5,   200] loss: 0.231\n",
      "[5,   400] loss: 0.231\n",
      "epoch [5/100], loss:0.2302\n",
      "[6,   200] loss: 0.231\n",
      "[6,   400] loss: 0.231\n",
      "epoch [6/100], loss:0.2307\n",
      "[7,   200] loss: 0.230\n",
      "[7,   400] loss: 0.230\n",
      "epoch [7/100], loss:0.2283\n",
      "[8,   200] loss: 0.230\n",
      "[8,   400] loss: 0.230\n",
      "epoch [8/100], loss:0.2297\n",
      "[9,   200] loss: 0.230\n",
      "[9,   400] loss: 0.230\n",
      "epoch [9/100], loss:0.2305\n",
      "[10,   200] loss: 0.229\n",
      "[10,   400] loss: 0.228\n",
      "epoch [10/100], loss:0.2276\n",
      "it took 238 seconds for 100 epochs\n",
      "[11,   200] loss: 0.227\n",
      "[11,   400] loss: 0.227\n",
      "epoch [11/100], loss:0.2276\n",
      "[12,   200] loss: 0.227\n",
      "[12,   400] loss: 0.227\n",
      "epoch [12/100], loss:0.2280\n",
      "[13,   200] loss: 0.227\n",
      "[13,   400] loss: 0.226\n",
      "epoch [13/100], loss:0.2249\n",
      "[14,   200] loss: 0.226\n",
      "[14,   400] loss: 0.226\n",
      "epoch [14/100], loss:0.2253\n",
      "[15,   200] loss: 0.225\n",
      "[15,   400] loss: 0.225\n",
      "epoch [15/100], loss:0.2240\n",
      "[16,   200] loss: 0.225\n",
      "[16,   400] loss: 0.225\n",
      "epoch [16/100], loss:0.2239\n",
      "[17,   200] loss: 0.225\n",
      "[17,   400] loss: 0.224\n",
      "epoch [17/100], loss:0.2241\n",
      "[18,   200] loss: 0.224\n",
      "[18,   400] loss: 0.224\n",
      "epoch [18/100], loss:0.2250\n",
      "[19,   200] loss: 0.224\n",
      "[19,   400] loss: 0.224\n",
      "epoch [19/100], loss:0.2242\n",
      "[20,   200] loss: 0.224\n",
      "[20,   400] loss: 0.224\n",
      "epoch [20/100], loss:0.2249\n",
      "it took 617 seconds for 100 epochs\n",
      "[21,   200] loss: 0.224\n",
      "[21,   400] loss: 0.224\n",
      "epoch [21/100], loss:0.2237\n",
      "[22,   200] loss: 0.224\n",
      "[22,   400] loss: 0.224\n",
      "epoch [22/100], loss:0.2232\n",
      "[23,   200] loss: 0.224\n",
      "[23,   400] loss: 0.224\n",
      "epoch [23/100], loss:0.2236\n",
      "[24,   200] loss: 0.223\n",
      "[24,   400] loss: 0.223\n",
      "epoch [24/100], loss:0.2226\n",
      "[25,   200] loss: 0.223\n",
      "[25,   400] loss: 0.223\n",
      "epoch [25/100], loss:0.2230\n",
      "[26,   200] loss: 0.223\n",
      "[26,   400] loss: 0.223\n",
      "epoch [26/100], loss:0.2222\n",
      "[27,   200] loss: 0.223\n",
      "[27,   400] loss: 0.223\n",
      "epoch [27/100], loss:0.2220\n",
      "[28,   200] loss: 0.222\n",
      "[28,   400] loss: 0.223\n",
      "epoch [28/100], loss:0.2244\n",
      "[29,   200] loss: 0.222\n",
      "[29,   400] loss: 0.222\n",
      "epoch [29/100], loss:0.2217\n",
      "[30,   200] loss: 0.222\n",
      "[30,   400] loss: 0.222\n",
      "epoch [30/100], loss:0.2231\n",
      "it took 1006 seconds for 100 epochs\n",
      "[31,   200] loss: 0.222\n",
      "[31,   400] loss: 0.222\n",
      "epoch [31/100], loss:0.2231\n",
      "[32,   200] loss: 0.222\n",
      "[32,   400] loss: 0.222\n",
      "epoch [32/100], loss:0.2224\n",
      "[33,   200] loss: 0.222\n",
      "[33,   400] loss: 0.222\n",
      "epoch [33/100], loss:0.2227\n",
      "[34,   200] loss: 0.222\n",
      "[34,   400] loss: 0.222\n",
      "epoch [34/100], loss:0.2246\n",
      "[35,   200] loss: 0.222\n",
      "[35,   400] loss: 0.222\n",
      "epoch [35/100], loss:0.2217\n",
      "[36,   200] loss: 0.222\n",
      "[36,   400] loss: 0.222\n",
      "epoch [36/100], loss:0.2223\n",
      "[37,   200] loss: 0.222\n",
      "[37,   400] loss: 0.222\n",
      "epoch [37/100], loss:0.2224\n",
      "[38,   200] loss: 0.222\n",
      "[38,   400] loss: 0.222\n",
      "epoch [38/100], loss:0.2227\n",
      "[39,   200] loss: 0.222\n",
      "[39,   400] loss: 0.222\n",
      "epoch [39/100], loss:0.2216\n",
      "[40,   200] loss: 0.222\n",
      "[40,   400] loss: 0.222\n",
      "epoch [40/100], loss:0.2214\n",
      "it took 1395 seconds for 100 epochs\n",
      "[41,   200] loss: 0.222\n",
      "[41,   400] loss: 0.222\n",
      "epoch [41/100], loss:0.2230\n",
      "[42,   200] loss: 0.222\n",
      "[42,   400] loss: 0.222\n",
      "epoch [42/100], loss:0.2215\n",
      "[43,   200] loss: 0.222\n",
      "[43,   400] loss: 0.222\n",
      "epoch [43/100], loss:0.2202\n",
      "[44,   200] loss: 0.222\n",
      "[44,   400] loss: 0.222\n",
      "epoch [44/100], loss:0.2224\n",
      "[45,   200] loss: 0.222\n",
      "[45,   400] loss: 0.222\n",
      "epoch [45/100], loss:0.2224\n",
      "[46,   200] loss: 0.222\n",
      "[46,   400] loss: 0.222\n",
      "epoch [46/100], loss:0.2210\n",
      "[47,   200] loss: 0.222\n",
      "[47,   400] loss: 0.222\n",
      "epoch [47/100], loss:0.2220\n",
      "[48,   200] loss: 0.222\n",
      "[48,   400] loss: 0.222\n",
      "epoch [48/100], loss:0.2221\n",
      "[49,   200] loss: 0.221\n",
      "[49,   400] loss: 0.221\n",
      "epoch [49/100], loss:0.2212\n",
      "[50,   200] loss: 0.221\n",
      "[50,   400] loss: 0.221\n",
      "epoch [50/100], loss:0.2201\n",
      "it took 1791 seconds for 100 epochs\n",
      "[51,   200] loss: 0.221\n",
      "[51,   400] loss: 0.221\n",
      "epoch [51/100], loss:0.2211\n",
      "[52,   200] loss: 0.221\n",
      "[52,   400] loss: 0.221\n",
      "epoch [52/100], loss:0.2201\n",
      "[53,   200] loss: 0.221\n",
      "[53,   400] loss: 0.221\n",
      "epoch [53/100], loss:0.2208\n",
      "[54,   200] loss: 0.221\n",
      "[54,   400] loss: 0.221\n",
      "epoch [54/100], loss:0.2218\n",
      "[55,   200] loss: 0.221\n",
      "[55,   400] loss: 0.221\n",
      "epoch [55/100], loss:0.2203\n",
      "[56,   200] loss: 0.221\n",
      "[56,   400] loss: 0.221\n",
      "epoch [56/100], loss:0.2211\n",
      "[57,   200] loss: 0.221\n",
      "[57,   400] loss: 0.221\n",
      "epoch [57/100], loss:0.2201\n",
      "[58,   200] loss: 0.221\n",
      "[58,   400] loss: 0.221\n",
      "epoch [58/100], loss:0.2215\n",
      "[59,   200] loss: 0.221\n",
      "[59,   400] loss: 0.221\n",
      "epoch [59/100], loss:0.2199\n",
      "[60,   200] loss: 0.221\n",
      "[60,   400] loss: 0.221\n",
      "epoch [60/100], loss:0.2214\n",
      "it took 2190 seconds for 100 epochs\n",
      "[61,   200] loss: 0.221\n",
      "[61,   400] loss: 0.221\n",
      "epoch [61/100], loss:0.2192\n",
      "[62,   200] loss: 0.221\n",
      "[62,   400] loss: 0.221\n",
      "epoch [62/100], loss:0.2203\n",
      "[63,   200] loss: 0.221\n",
      "[63,   400] loss: 0.221\n",
      "epoch [63/100], loss:0.2209\n",
      "[64,   200] loss: 0.221\n",
      "[64,   400] loss: 0.221\n",
      "epoch [64/100], loss:0.2219\n",
      "[65,   200] loss: 0.221\n",
      "[65,   400] loss: 0.221\n",
      "epoch [65/100], loss:0.2217\n",
      "[66,   200] loss: 0.221\n",
      "[66,   400] loss: 0.221\n",
      "epoch [66/100], loss:0.2206\n",
      "[67,   200] loss: 0.221\n",
      "[67,   400] loss: 0.221\n",
      "epoch [67/100], loss:0.2215\n",
      "[68,   200] loss: 0.221\n",
      "[68,   400] loss: 0.221\n",
      "epoch [68/100], loss:0.2215\n",
      "[69,   200] loss: 0.221\n",
      "[69,   400] loss: 0.221\n",
      "epoch [69/100], loss:0.2196\n",
      "[70,   200] loss: 0.221\n",
      "[70,   400] loss: 0.221\n",
      "epoch [70/100], loss:0.2225\n",
      "it took 2607 seconds for 100 epochs\n",
      "[71,   200] loss: 0.221\n",
      "[71,   400] loss: 0.221\n",
      "epoch [71/100], loss:0.2190\n",
      "[72,   200] loss: 0.221\n",
      "[72,   400] loss: 0.221\n",
      "epoch [72/100], loss:0.2201\n",
      "[73,   200] loss: 0.221\n",
      "[73,   400] loss: 0.221\n",
      "epoch [73/100], loss:0.2207\n",
      "[74,   200] loss: 0.221\n",
      "[74,   400] loss: 0.221\n",
      "epoch [74/100], loss:0.2197\n",
      "[75,   200] loss: 0.221\n",
      "[75,   400] loss: 0.221\n",
      "epoch [75/100], loss:0.2217\n",
      "[76,   200] loss: 0.220\n",
      "[76,   400] loss: 0.221\n",
      "epoch [76/100], loss:0.2196\n",
      "[77,   200] loss: 0.221\n",
      "[77,   400] loss: 0.220\n",
      "epoch [77/100], loss:0.2201\n",
      "[78,   200] loss: 0.221\n",
      "[78,   400] loss: 0.221\n",
      "epoch [78/100], loss:0.2206\n",
      "[79,   200] loss: 0.220\n",
      "[79,   400] loss: 0.221\n",
      "epoch [79/100], loss:0.2186\n",
      "[80,   200] loss: 0.220\n",
      "[80,   400] loss: 0.220\n",
      "epoch [80/100], loss:0.2210\n",
      "it took 3022 seconds for 100 epochs\n",
      "[81,   200] loss: 0.221\n",
      "[81,   400] loss: 0.221\n",
      "epoch [81/100], loss:0.2201\n",
      "[82,   200] loss: 0.220\n",
      "[82,   400] loss: 0.221\n",
      "epoch [82/100], loss:0.2227\n",
      "[83,   200] loss: 0.220\n",
      "[83,   400] loss: 0.221\n",
      "epoch [83/100], loss:0.2210\n",
      "[84,   200] loss: 0.220\n",
      "[84,   400] loss: 0.220\n",
      "epoch [84/100], loss:0.2188\n",
      "[85,   200] loss: 0.220\n",
      "[85,   400] loss: 0.221\n",
      "epoch [85/100], loss:0.2210\n",
      "[86,   200] loss: 0.220\n",
      "[86,   400] loss: 0.220\n",
      "epoch [86/100], loss:0.2210\n",
      "[87,   200] loss: 0.221\n",
      "[87,   400] loss: 0.220\n",
      "epoch [87/100], loss:0.2208\n",
      "[88,   200] loss: 0.220\n",
      "[88,   400] loss: 0.220\n",
      "epoch [88/100], loss:0.2197\n",
      "[89,   200] loss: 0.220\n",
      "[89,   400] loss: 0.220\n",
      "epoch [89/100], loss:0.2215\n",
      "[90,   200] loss: 0.220\n",
      "[90,   400] loss: 0.221\n",
      "epoch [90/100], loss:0.2207\n",
      "it took 3434 seconds for 100 epochs\n",
      "[91,   200] loss: 0.220\n",
      "[91,   400] loss: 0.220\n",
      "epoch [91/100], loss:0.2211\n",
      "[92,   200] loss: 0.220\n",
      "[92,   400] loss: 0.220\n",
      "epoch [92/100], loss:0.2192\n",
      "[93,   200] loss: 0.220\n",
      "[93,   400] loss: 0.220\n",
      "epoch [93/100], loss:0.2203\n",
      "[94,   200] loss: 0.220\n",
      "[94,   400] loss: 0.220\n",
      "epoch [94/100], loss:0.2211\n",
      "[95,   200] loss: 0.220\n",
      "[95,   400] loss: 0.220\n",
      "epoch [95/100], loss:0.2199\n",
      "[96,   200] loss: 0.220\n",
      "[96,   400] loss: 0.220\n",
      "epoch [96/100], loss:0.2198\n",
      "[97,   200] loss: 0.220\n",
      "[97,   400] loss: 0.220\n",
      "epoch [97/100], loss:0.2207\n",
      "[98,   200] loss: 0.220\n",
      "[98,   400] loss: 0.220\n",
      "epoch [98/100], loss:0.2206\n",
      "[99,   200] loss: 0.220\n",
      "[99,   400] loss: 0.220\n",
      "epoch [99/100], loss:0.2201\n",
      "[100,   200] loss: 0.220\n",
      "[100,   400] loss: 0.220\n",
      "epoch [100/100], loss:0.2225\n",
      "it took 3844 seconds for 100 epochs\n",
      "it took 3844 seconds to finish 100 epochs\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        img, _ = data\n",
    "        \n",
    "        output = autoencoder(img)\n",
    "        loss = distance(output, img)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 200))\n",
    "            running_loss = 0.0\n",
    "        \n",
    "    # ===================log========================\n",
    "    print('epoch [{}/{}], loss:{:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "    if epoch % 10 == 9:\n",
    "        print('it took %d seconds for %d epochs' % (time.time() - start, num_epochs))\n",
    "print('it took %d seconds to finish %d epochs' % (time.time() - start, num_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "with torch.no_grad():\n",
    "    outputs = autoencoder(example_data)\n",
    "    print(distance(example_data, outputs).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "for i in range(50):\n",
    "    plt.subplot(5, 10, i+1)\n",
    "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"real %d\" % example_targets[i])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "for i in range(50):\n",
    "    plt.subplot(5, 10, i + 1)\n",
    "    plt.imshow(outputs[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"%d\" % example_targets[i])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you would like to write some notes, save it as a string and pass it to the 'message' parameter of save_model\n",
    "# if you don't set message to None\n",
    "message = '''This was the first model that seemed to work. Loss started at about .231 and went to .219 after 100 epcohs.\n",
    "however, the generated images don't look too bad.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(autoencoder, message=message, fig_to_save=fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (CS269Q)",
   "language": "python",
   "name": "cs269q"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
